{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E87bI9Rnqa2c"
      },
      "source": [
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoTqlnMhOSji",
        "outputId": "9bf01580-e9f2-44a8-b0a5-f371260bf262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting captum\n",
            "  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from captum) (3.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from captum) (1.22.4)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.8/dist-packages (from captum) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->captum) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (23.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (4.38.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->captum) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.15.0)\n",
            "Installing collected packages: captum\n",
            "Successfully installed captum-0.6.0\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "writer = SummaryWriter(\"my_dir\")\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import librosa\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchsummary import summary\n",
        "import torchaudio.transforms as T\n",
        "from sklearn.decomposition import PCA\n",
        "!pip install captum\n",
        "from captum.attr import (\n",
        "    GradientShap,\n",
        "    DeepLift,\n",
        "    DeepLiftShap,\n",
        "    IntegratedGradients,\n",
        "    LayerConductance,\n",
        "    NeuronConductance,\n",
        "    NoiseTunnel,\n",
        ")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZBdkZQ6qgF9"
      },
      "source": [
        "Creating visualization functions for the audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVahnAS8g-LP"
      },
      "outputs": [],
      "source": [
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Spectrogram (db)')\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  plt.show(block=False)\n",
        "  plt.show()\n",
        "\n",
        "def plot_pitch(waveform, sample_rate, pitch):\n",
        "  figure, axis = plt.subplots(1, 1)\n",
        "  axis.set_title(\"Pitch Feature\")\n",
        "  axis.grid(True)\n",
        "\n",
        "  end_time = waveform.shape[1] / sample_rate\n",
        "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
        "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
        "\n",
        "  axis2 = axis.twinx()\n",
        "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "  ln2 = axis2.plot(\n",
        "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
        "\n",
        "  axis2.legend(loc=0)\n",
        "  plt.show(block=False)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex0m5ksJqqiw"
      },
      "source": [
        "Initializes data modules for the audio to be preprocessed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY-OWSuXqwK_"
      },
      "source": [
        "Initializing model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6axHZ6cqCLTY"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 conv blocks / flatten / linear / softmax\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(21120, 1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear(x)\n",
        "        return logits\n",
        "\n",
        "class RNNNetwork (nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "        super(RNNNetwork, self).__init__()\n",
        "        self.num_classes = num_classes #number of classes\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "        self.seq_length = seq_length #sequence length\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
        "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #hidden state\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(device) #internal state\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        out = self.fc_1(out) #first Dense\n",
        "        out = self.relu(out) #relu\n",
        "        out = self.fc(out) #Final Output\n",
        "        return out\n",
        " \n",
        "\n",
        "class CRNN(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.num_classes = num_classes #number of classes\n",
        "    self.num_layers = num_layers #number of layers\n",
        "    self.input_size = input_size #input size\n",
        "    self.hidden_size = hidden_size #hidden state\n",
        "    self.seq_length = seq_length #sequence length\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "    self.fc1 = nn.Linear(660, num_classes) #fully connected last layer\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "            #print(x.shape)\n",
        "            x = self.conv1(x)\n",
        "            #print(x.shape)\n",
        "            x = self.conv2(x)\n",
        "            #print(x.shape)\n",
        "            x = self.conv3(x)\n",
        "            #print(x.shape)\n",
        "            x = self.conv4(x)\n",
        "            #print(x.shape)\n",
        "            x = x.transpose(1, -1)\n",
        "            #print(x.shape)\n",
        "            batch, time = x.size()[:2]\n",
        "            x = x.reshape(batch, time, -1)\n",
        "            h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda() #hidden state\n",
        "            c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda() #internal state\n",
        "           # Propagate input through LSTM\n",
        "            #print(x.shape)\n",
        "            output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "            #print(output.shape)\n",
        "            hn = torch.reshape(output, (output.size(0), -1))\n",
        "            #print(hn.shape)\n",
        "            x = self.relu(hn)\n",
        "            #print(x.shape)\n",
        "            x = self.fc1(x)\n",
        "            #print(x.shape)\n",
        "            return x\n",
        "\n",
        "class MergeCRNN(nn.Module):\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "    self.num_classes = num_classes #number of classes\n",
        "    self.num_layers = num_layers #number of layers\n",
        "    self.input_size = input_size #input size\n",
        "    self.hidden_size = hidden_size #hidden state\n",
        "    self.seq_length = seq_length #sequence length\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "    self.fc1 = nn.Linear(660, num_classes) #fully connected last layer\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "            #print(x.shape)\n",
        "            conv_output = self.conv1(x)\n",
        "            #print(x.shape)\n",
        "            conv_output = self.conv2(conv_output)\n",
        "            #print(x.shape)\n",
        "            conv_output = self.conv3(conv_output)\n",
        "            #print(x.shape)\n",
        "            conv_output = self.conv4(conv_output)\n",
        "            conv_output = conv_output.transpose(1, -1)\n",
        "            batch, time = x.size()[:2]\n",
        "            lstm_output = x.reshape(batch, time, -1)\n",
        "            print(lstm_output.shape)\n",
        "            h_0 = Variable(torch.zeros(self.num_layers, lstm_output.size(0), self.hidden_size)).cuda() #hidden state\n",
        "            c_0 = Variable(torch.zeros(self.num_layers, lstm_output.size(0), self.hidden_size)).cuda() #internal state\n",
        "           # Propagate input through LSTM\n",
        "            #print(x.shape)\n",
        "            lstm_output, (hn, cn) = self.lstm(lstm_output, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "            #print(output.shape)\n",
        "            output = torch.cat(torch.reshape(lstm_output, (output.size(0), -1)),torch.reshape(conv_output, (output.size(0), -1)))\n",
        "            #print(hn.shape)\n",
        "            output = self.relu(output)\n",
        "            #print(x.shape)\n",
        "            output = self.fc1(output)\n",
        "            #print(x.shape)\n",
        "            return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the datasets for preprocessing the raw binary labelled data\n"
      ],
      "metadata": {
        "id": "jBlVTtHEgZRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rD4j7ls_T0yM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb52393-eeb7-423d-8651-c3be2ee80109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A corrupt file was found\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n",
            "dysarthria\n"
          ]
        }
      ],
      "source": [
        "from scipy.io.wavfile import write\n",
        "import random\n",
        "from IPython.display import Audio, display\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class DysarthriaBinaryDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        self.data = pd.read_csv(root_dir + '/data.csv')\n",
        "        self.mat = []\n",
        "        def preprocess(signal, sr, augument=False, visualize=False):\n",
        "                if augument:\n",
        "                    effects = [\n",
        "                        [\"speed\", str(random.uniform(0.5, 1.5))],\n",
        "                        ['pitch', str(random.uniform(-8,8))],\n",
        "                        [\"rate\", f\"{sr}\"],\n",
        "                    ]\n",
        "                    signal, sr = torchaudio.sox_effects.apply_effects_tensor(signal, sr, effects)\n",
        "                if sr != 16000:\n",
        "                    resampler = torchaudio.transforms.Resample(sr, 16000)\n",
        "                    signal = resampler(signal)\n",
        "                if signal.shape[1] > 100000:\n",
        "                    signal = signal[:, :100000]\n",
        "                length_signal = signal.shape[1]\n",
        "                if length_signal < 100000:\n",
        "                    num_missing_samples = 100000 - length_signal\n",
        "                    last_dim_padding = (0, num_missing_samples)\n",
        "                    signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "                if visualize:\n",
        "                  display(Audio(signal, rate=16000))\n",
        "                mfcc = T.MFCC(sample_rate=16000, n_mfcc=64, melkwargs={\"n_mels\": 64})\n",
        "                signal = mfcc(signal)\n",
        "                if visualize:\n",
        "                  plot_spectrogram(signal[0])\n",
        "                return signal\n",
        " \n",
        "        for index in range(2000):\n",
        "            try:\n",
        "                label = torch.tensor([1.0]) if self.data.iloc[index][0] == 'dysarthria' else torch.tensor([0.0])\n",
        "                signal, sr = torchaudio.load('drive/MyDrive/' + self.data.iloc[index][2])\n",
        "                #if label == torch.tensor([1.0]):\n",
        "                  #samp1 = preprocess(signal, sr, visualize=True)\n",
        "                  #samp2 = preprocess(signal, sr, augument=True, visualize=True)\n",
        "                  #samp3 = preprocess(signal, sr, augument=True, visualize=True)\n",
        "                  #samp4 = preprocess(signal, sr, augument=True, visualize=True)\n",
        "                #else:\n",
        "                samp1 = preprocess(signal, sr)\n",
        "                samp2 = preprocess(signal, sr, augument=True)\n",
        "                samp3 = preprocess(signal, sr, augument=True)\n",
        "                samp4 = preprocess(signal, sr, augument=True)\n",
        "                self.mat.append([samp1, label]), self.mat.append([samp2, label]), self.mat.append([samp3, label]), self.mat.append([samp4, label])\n",
        "            except:\n",
        "                print('A corrupt file was found')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mat)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.mat[index][0], self.mat[index][1]\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using {device}\")\n",
        "data = DysarthriaBinaryDataset('drive/MyDrive/torgo_data')\n",
        "print(len(data))\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(data, [7396,300, 300])\n",
        "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing training modules for CNN"
      ],
      "metadata": {
        "id": "4iDFN4ahhWyj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiO8khJru8Jx",
        "outputId": "654f605e-f880-4291-94dc-0cca10aa3a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNNetwork(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear): Linear(in_features=21120, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "LEARNING_RATE = 2e-3\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = CNNNetwork().to(device)\n",
        "\n",
        "print(cnn)\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(cnn.parameters(), lr=LEARNING_RATE)\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max = 50, # Maximum number of iterations.\n",
        "                             eta_min = 8e-4) # Minimum learning rate.\n",
        "for param in cnn.parameters():\n",
        "    param.requires_grad = True\n",
        "# train model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training cnn model"
      ],
      "metadata": {
        "id": "xYqKgAj6hkNr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuBDOg4wMB_0",
        "outputId": "01943874-0843-4bdb-8f26-f99363ea4da5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "3.771016664637818 0.5726701275507609 61.727420227149814 65.30666666666667\n",
            "Epoch 2\n",
            "0.5850510439110679 0.5434689275423685 68.19307733910222 68.50666666666666\n",
            "Epoch 3\n",
            "0.5624621707790926 0.5410633937517801 70.0 73.10666666666667\n",
            "Epoch 4\n",
            "0.5475646524638211 0.5228439104557038 71.59816116819903 71.4\n",
            "Epoch 5\n",
            "0.5314718891054053 0.5722803052266439 71.92861005949162 75.41333333333333\n",
            "Epoch 6\n",
            "0.5291684659640167 0.5413022565841675 72.89616008653326 77.04\n",
            "Epoch 7\n",
            "0.5203121045011001 0.49889849265416464 73.40400216333153 71.84\n",
            "Epoch 8\n",
            "0.5140150171025113 0.4920367439587911 74.06544077879936 71.14666666666666\n",
            "Epoch 9\n",
            "0.5051360623523182 0.48512478431065875 74.559221200649 72.25333333333333\n",
            "Epoch 10\n",
            "0.49533722969955724 0.47404871384302777 75.23472147106544 76.14666666666666\n",
            "Epoch 11\n",
            "0.4890952963581467 0.471172624429067 75.64954029204975 76.10666666666667\n",
            "Epoch 12\n",
            "0.4797003045799024 0.4573003149032593 75.97890751757707 74.6\n",
            "Epoch 13\n",
            "0.4759876250801633 0.45994997302691143 76.69280692266089 77.98666666666666\n",
            "Epoch 14\n",
            "0.46412901927678246 0.4442600071430206 76.67712276906435 75.46666666666667\n",
            "Epoch 15\n",
            "0.45819763838438937 0.4662242269515991 77.18875067604111 78.06666666666666\n",
            "Epoch 16\n",
            "0.4787156264030205 0.43882797916730243 76.90968090859924 76.86666666666666\n",
            "Epoch 17\n",
            "0.45260797952754617 0.44210042476654055 78.2120064899946 78.88\n",
            "Epoch 18\n",
            "0.43590262314640166 0.4297542103131612 79.01730665224446 76.52\n",
            "Epoch 19\n",
            "0.4441366546408043 0.4319776153564453 78.76527852893456 78.44\n",
            "Epoch 20\n",
            "0.4303456824685897 0.436977748076121 79.04218496484586 77.56\n",
            "Epoch 21\n",
            "0.42386805627589746 0.4020450790723165 79.76744186046511 79.73333333333333\n",
            "Epoch 22\n",
            "0.4164661377942388 0.40526383797327675 80.55164954029205 78.10666666666667\n",
            "Epoch 23\n",
            "0.409704542630682 0.4121907305717468 80.80800432666307 79.72\n",
            "Epoch 24\n",
            "0.3986781047220937 0.37982852578163145 81.08491076257437 82.45333333333333\n",
            "Epoch 25\n",
            "0.39522680806236826 0.43888060371081034 81.4310438074635 81.44\n",
            "Epoch 26\n",
            "0.3917777327636308 0.3697497304280599 81.72525689561925 81.17333333333333\n",
            "Epoch 27\n",
            "0.39109442563170416 0.37937825481096904 82.27474310438075 78.44\n",
            "Epoch 28\n",
            "0.3813661911830572 0.3666670552889506 82.55976203353164 81.86666666666666\n",
            "Epoch 29\n",
            "0.3747182261660139 0.3818260391553243 83.04434829637643 77.62666666666667\n",
            "Epoch 30\n",
            "0.36739943835334304 0.3520118228594462 83.39426717144403 82.25333333333333\n",
            "Epoch 31\n",
            "0.3676023241182867 0.35156309763590493 83.48458626284479 83.56\n",
            "Epoch 32\n",
            "0.3561306899466858 0.34340792258580527 83.89778258518118 82.76\n",
            "Epoch 33\n",
            "0.34999126595635877 0.35168777068456014 84.12709572742023 82.84\n",
            "Epoch 34\n",
            "0.35044851048112624 0.335037491718928 84.09843158464035 84.58666666666667\n",
            "Epoch 35\n",
            "0.33921873116570594 0.35304535150527955 84.69064359113034 84.0\n",
            "Epoch 36\n",
            "0.3422875014784015 0.35515809973080953 84.967009194159 79.46666666666667\n",
            "Epoch 37\n",
            "0.332212915737736 0.3848406664530436 85.30340724716062 75.46666666666667\n",
            "Epoch 38\n",
            "0.33704269209573434 0.3649090282122294 85.38020551649541 77.0\n",
            "Epoch 39\n",
            "0.3311240332006699 0.32384751915931703 85.9021092482423 84.29333333333334\n",
            "Epoch 40\n",
            "0.3292838084078789 0.3349510729312897 85.89616008653326 85.26666666666667\n",
            "Epoch 41\n",
            "0.3190846633691927 0.3229359451929728 86.3401838831801 81.69333333333333\n",
            "Epoch 42\n",
            "0.3083015267368005 0.29909919381141664 86.64413196322336 85.14666666666666\n",
            "Epoch 43\n",
            "0.30819217432763785 0.34430458704630534 86.38831800973499 79.69333333333333\n",
            "Epoch 44\n",
            "0.3125661922474691 0.3166906992594401 86.41157382368849 82.84\n",
            "Epoch 45\n",
            "0.3017336797752917 0.30369571069876355 86.7479718766901 83.73333333333333\n",
            "Epoch 46\n",
            "0.29605624607571274 0.30073247452576957 87.25040562466198 85.30666666666667\n",
            "Epoch 47\n",
            "0.2979961074223836 0.29842403133710227 86.99459167117361 85.82666666666667\n",
            "Epoch 48\n",
            "0.29527622617664306 0.33510583559672036 87.3899405083829 80.41333333333333\n",
            "Epoch 49\n",
            "0.2960502708873728 0.2881757519642512 87.43645213628989 85.44\n",
            "Epoch 50\n",
            "0.28618553111590583 0.2826871172587077 87.58842617631151 85.44\n",
            "Epoch 51\n",
            "0.2882916205570207 0.28146805504957834 87.59816116819903 83.73333333333333\n",
            "Epoch 52\n",
            "0.29071181811082036 0.2890345710515976 87.41373715521904 84.72\n",
            "Epoch 53\n",
            "0.28609788790143714 0.26855494260787965 87.73445105462412 85.69333333333333\n",
            "Epoch 54\n",
            "0.283888121145103 0.27802167435487113 88.01730665224446 87.45333333333333\n",
            "Epoch 55\n",
            "0.28854866863393086 0.26638515830039977 87.5868036776636 85.69333333333333\n",
            "Epoch 56\n",
            "0.28566474705660516 0.3110643619298935 87.9518658734451 83.73333333333333\n",
            "Epoch 57\n",
            "0.2823928208077773 0.2861558582385381 88.05408328826393 89.45333333333333\n",
            "Epoch 58\n",
            "0.27641097920014446 0.26141994615395864 87.99621416982153 86.17333333333333\n",
            "Epoch 59\n",
            "0.27528635223470166 0.29745855371157326 87.52514872904273 83.44\n",
            "Epoch 60\n",
            "0.28712142764587156 0.26329787691434225 87.91941590048675 86.97333333333333\n",
            "Epoch 61\n",
            "0.27066224895663493 0.25927507956822715 88.47701460248783 87.86666666666666\n",
            "Epoch 62\n",
            "0.27177549409440815 0.30807929356892905 88.36181719848567 86.42666666666666\n",
            "Epoch 63\n",
            "0.27302073169521796 0.2670149644215902 88.61763115197404 88.94666666666667\n",
            "Epoch 64\n",
            "0.26836261104866904 0.2568236299355825 88.24445646295295 85.69333333333333\n",
            "Epoch 65\n",
            "0.29728948056955734 0.26938157002131147 87.87344510546241 84.97333333333333\n",
            "Epoch 66\n",
            "0.2648309034840876 0.2625131326913834 88.69821525148728 89.88\n",
            "Epoch 67\n",
            "0.2566389835012997 0.2507158863544464 88.74634937804218 86.12\n",
            "Epoch 68\n",
            "0.28334089950847263 0.2673718802134196 88.47485127095727 84.84\n",
            "Epoch 69\n",
            "0.2624932371035597 0.27516602277755736 88.62898864250947 83.73333333333333\n",
            "Epoch 70\n",
            "0.269337137722853 0.2387188321352005 88.44456462952948 87.88\n",
            "Epoch 71\n",
            "0.2505901541949737 0.2397756000359853 89.46511627906976 86.54666666666667\n",
            "Epoch 72\n",
            "0.27718600972463403 0.25090741137663525 87.88480259599784 87.57333333333334\n",
            "Epoch 73\n",
            "0.25309393527122237 0.26323493440945944 89.16765819361817 88.0\n",
            "Epoch 74\n",
            "0.2637782851269337 0.24010905702908833 88.95402920497567 86.29333333333334\n",
            "Epoch 75\n",
            "0.257952525244654 0.23485872983932496 89.21579232017307 88.17333333333333\n",
            "Epoch 76\n",
            "0.24811880726502225 0.2668741257985433 89.42130881557598 90.73333333333333\n",
            "Epoch 77\n",
            "0.2745801661226026 0.24925479392210642 88.86316928069226 86.58666666666667\n",
            "Epoch 78\n",
            "0.2572363432740572 0.42107391913731895 89.2974580854516 76.53333333333333\n",
            "Epoch 79\n",
            "0.24161401773027757 0.31761791586875915 89.74743104380747 80.97333333333333\n",
            "Epoch 80\n",
            "0.2509050660677508 0.25760743180910745 89.47322877230935 90.13333333333334\n",
            "Epoch 81\n",
            "0.24946088620686674 0.30922833442687986 90.15792320173067 83.09333333333333\n",
            "Epoch 82\n",
            "0.2363608338144291 0.22754693587621053 90.03677663601947 91.41333333333333\n",
            "Epoch 83\n",
            "0.24548639214445026 0.2836001801490784 89.99026500811249 89.41333333333333\n",
            "Epoch 84\n",
            "0.22405708679932398 0.28532427151997886 91.20930232558139 84.88\n",
            "Epoch 85\n",
            "0.24542456511518904 0.23584049820899963 89.99783666846945 85.69333333333333\n",
            "Epoch 86\n",
            "0.23926359290945137 0.22469663500785828 90.40021633315305 88.0\n",
            "Epoch 87\n",
            "0.2126838933493138 0.3455562082926432 91.3401838831801 89.41333333333333\n",
            "Epoch 88\n",
            "0.21545771114113138 0.5216662951310476 91.17036235803137 72.42666666666666\n",
            "Epoch 89\n",
            "0.21925878215313216 0.2803332583109538 91.56138453217956 85.86666666666666\n",
            "Epoch 90\n",
            "0.21299840761333494 0.21312719861666363 91.76365603028664 88.0\n",
            "Epoch 91\n",
            "0.21163720982083764 0.20990811844666799 91.3250405624662 87.44\n",
            "Epoch 92\n",
            "0.21161802436223606 0.2676927252610524 91.4310438074635 92.56\n",
            "Epoch 93\n",
            "0.18964857307074842 0.2277369753519694 92.18442401297999 88.68\n",
            "Epoch 94\n",
            "0.20507241124685163 0.24588558812936148 91.74256354786371 92.86666666666666\n",
            "Epoch 95\n",
            "0.17873373979036714 0.3965638037522634 92.59707950243374 79.29333333333334\n",
            "Epoch 96\n",
            "0.20546123345537146 0.20083069483439128 91.85992428339642 90.86666666666666\n",
            "Epoch 97\n",
            "0.18849292510728569 0.21656197826067605 92.81179015684154 93.0\n",
            "Epoch 98\n",
            "0.2042056036082521 0.19040448149045308 91.9269875608437 90.56\n",
            "Epoch 99\n",
            "0.16254851908668302 0.1875309693813324 93.51541373715521 90.86666666666666\n",
            "Epoch 100\n",
            "0.1824798636448712 0.20199123442173003 93.02325581395348 92.14666666666666\n",
            "Epoch 101\n",
            "0.1729905876825151 0.1853111716111501 93.64034613304489 89.57333333333334\n",
            "Epoch 102\n",
            "0.1671493293031478 0.27613298575083417 93.88047593293672 86.97333333333333\n",
            "Epoch 103\n",
            "0.1712217511132706 0.17046926230192183 93.86371011357491 90.73333333333333\n",
            "Epoch 104\n",
            "0.1504129196435068 0.1849597551425298 94.20010816657653 93.0\n",
            "Epoch 105\n",
            "0.14719776274095425 0.24241866827011108 94.46349378042186 87.01333333333334\n",
            "Epoch 106\n",
            "0.19684364498789855 0.18535703430573144 92.82531097890752 90.61333333333333\n",
            "Epoch 107\n",
            "0.15932600887612178 0.18592102070649466 94.2271498107085 90.86666666666666\n",
            "Epoch 108\n",
            "0.14689275486976022 0.16576474050680795 94.69226608977826 90.86666666666666\n",
            "Epoch 109\n",
            "0.13414217089239877 0.16444751063982646 94.98269334775554 92.73333333333333\n",
            "Epoch 110\n",
            "0.1634214736356034 0.1860251490275065 93.7809626825311 90.56\n",
            "Epoch 111\n",
            "0.131105147040232 0.180766938328743 95.26933477555436 90.0\n",
            "Epoch 112\n",
            "0.12530728602422259 0.17278016010920208 95.4461871281774 91.02666666666667\n",
            "Epoch 113\n",
            "0.13298181970258738 0.19009068191051484 94.84802595997837 89.14666666666666\n",
            "Epoch 114\n",
            "0.139850598386715 0.1700043155749639 94.92320173066523 92.32\n",
            "Epoch 115\n",
            "0.11603245024280719 0.17093809843063354 95.87506760411033 91.16\n",
            "Epoch 116\n",
            "0.11264559242566126 0.15701163788636527 95.94050838290968 93.41333333333333\n",
            "Epoch 117\n",
            "0.11327928005681932 0.15258852461973826 95.82314764737696 93.41333333333333\n",
            "Epoch 118\n",
            "0.1066447316254391 0.16855805327494938 96.29421308815576 91.16\n",
            "Epoch 119\n",
            "0.10845262120065333 0.21518062591552733 95.95565170362357 88.42666666666666\n",
            "Epoch 120\n",
            "0.12092898192116865 0.17746392945448558 95.5770686857761 93.0\n",
            "Epoch 121\n",
            "0.0982503292164395 0.1653675095240275 96.58464034613304 93.29333333333334\n",
            "Epoch 122\n",
            "0.10189955955166763 0.14690304577350616 96.32125473228773 93.12\n",
            "Epoch 123\n",
            "0.10286703156967174 0.15741092423597972 96.26933477555436 93.42666666666666\n",
            "Epoch 124\n",
            "0.10225648565541222 0.1511657056212425 96.3731746890211 94.44\n",
            "Epoch 125\n",
            "0.09328507827448806 0.16741774439811707 96.68469442942131 93.41333333333333\n",
            "Epoch 126\n",
            "0.09814995001012278 0.15560760974884033 96.72687939426717 92.26666666666667\n",
            "Epoch 127\n",
            "0.09235995488917266 0.180014684398969 96.61546782044348 90.42666666666666\n",
            "Epoch 128\n",
            "0.08576664572763984 0.14549895450472833 97.00594916170903 91.02666666666667\n",
            "Epoch 129\n",
            "0.08500746065794035 0.17103041470050812 97.14440237966468 90.30666666666667\n",
            "Epoch 130\n",
            "0.0843762279870447 0.13926547547181448 97.09626825310978 93.0\n",
            "Epoch 131\n",
            "0.07998176522677883 0.15615395744641622 97.203893996755 91.45333333333333\n",
            "Epoch 132\n",
            "0.08284963611180941 0.14792325248320898 97.28664142779881 92.44\n",
            "Epoch 133\n",
            "0.08555788592334178 0.1700413578748703 97.35586803677664 95.0\n",
            "Epoch 134\n",
            "0.08018517587283548 0.14776678721110026 97.18658734451054 91.72\n",
            "Epoch 135\n",
            "0.07925697086855422 0.1458718506495158 97.43645213628989 93.85333333333334\n",
            "Epoch 136\n",
            "0.07662845659636368 0.14674519618352255 97.58085451595457 93.85333333333334\n",
            "Epoch 137\n",
            "0.07285388772264566 0.14980381608009338 97.66143861546782 92.10666666666667\n",
            "Epoch 138\n",
            "0.07611164441176077 0.13530232270558676 97.51162790697674 94.70666666666666\n",
            "Epoch 139\n",
            "0.07409034221921179 0.13220605542262395 97.62898864250947 94.44\n",
            "Epoch 140\n",
            "0.07505939202962404 0.14791038687030475 97.47485127095727 93.6\n",
            "Epoch 141\n",
            "0.07069306139642319 0.12974318861961365 97.57111952406706 94.10666666666667\n",
            "Epoch 142\n",
            "0.06840691944355576 0.13925522526105244 97.55002704164413 93.42666666666666\n",
            "Epoch 143\n",
            "0.07112304245768011 0.1363697737455368 97.7230935640887 94.10666666666667\n",
            "Epoch 144\n",
            "0.0674799821803446 0.1343710587422053 97.75770686857761 93.42666666666666\n",
            "Epoch 145\n",
            "0.06710273726743063 0.13291874885559082 97.73661438615468 93.42666666666666\n",
            "Epoch 146\n",
            "0.06604312616161168 0.16294801910718282 97.6652244456463 92.98666666666666\n",
            "Epoch 147\n",
            "0.06533567636616815 0.13015012602011364 97.8058409951325 93.42666666666666\n",
            "Epoch 148\n",
            "0.06427171173580534 0.12426673303047817 97.88263926446729 94.57333333333334\n",
            "Epoch 149\n",
            "0.06398739497514272 0.12725881059964497 97.90968090859924 93.0\n",
            "Epoch 150\n",
            "0.06808263040059576 0.1355994733174642 97.93834505137913 93.41333333333333\n",
            "---------------------------\n",
            "Finished training\n",
            "Trained feed forward net saved at cnnnetwork.pth\n"
          ]
        }
      ],
      "source": [
        "def calc_accuracy(mdl: torch.nn.Module, X: torch.Tensor, Y: torch.Tensor) -> float:\n",
        "        predicted = mdl(X)\n",
        "        correct = ((predicted > 0.5) == Y).sum().item()\n",
        "        return 100 * correct // len(Y)\n",
        "\n",
        "EPOCHS = 150\n",
        "for i in range(EPOCHS):\n",
        "    #try:\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        train_acc = 0\n",
        "        val_acc = 0\n",
        "        n = 0\n",
        "        for input, target in train_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = cnn(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            train_loss = train_loss + (loss.item()*len(input))\n",
        "            # backpropagate error and update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_acc = train_acc + (len(input)*calc_accuracy(cnn, input, target))\n",
        "            n = n + len(input)\n",
        "        train_loss/=n\n",
        "        train_acc/=n\n",
        "        n = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = cnn(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            val_loss = val_loss + (loss.item()*len(input))\n",
        "            val_acc = val_acc + (len(input)*calc_accuracy(cnn, input, target))\n",
        "            n = n + len(input)\n",
        "        val_loss/=n\n",
        "        val_acc/=n\n",
        "        writer.add_scalar('CNN_Loss/train', train_loss, i)\n",
        "        writer.add_scalar('CNN_Loss/validation', val_loss, i)\n",
        "        writer.add_scalar('CNN_Accuracy/train', train_acc, i)\n",
        "        writer.add_scalar('CNN_Accuracy/validation', val_acc, i)\n",
        "        print(train_loss, val_loss, train_acc, val_acc)\n",
        "        scheduler.step()\n",
        "    #except:\n",
        "    #  break\n",
        "print(\"---------------------------\")\n",
        "print(\"Finished training\")\n",
        "# save model\n",
        "torch.save(cnn.state_dict(), \"cnnnetwork.pth\")\n",
        "print(\"Trained feed forward net saved at cnnnetwork.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determining the importance of each input feature"
      ],
      "metadata": {
        "id": "Niw_AqN2hzdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#g = []\n",
        "#for m in range(50):\n",
        "#  model = CNNNetwork()\n",
        "#  model.load_state_dict(torch.load(\"dysarthriaClassifier.pth\"))\n",
        "#  ig = IntegratedGradients(model)\n",
        "#  input = next(iter(val_dataloader))\n",
        "  #input = torch.randn((1,1,64,501))\n",
        "#  attributions = ig.attribute(inputs=input[0][0:1], target=0)\n",
        "  #attributions = ig.attribute(inputs=input, target=0)\n",
        "#  attributions= torch.reshape(attributions, (64*501,))\n",
        "#  a = attributions.detach().cpu().numpy()\n",
        "#  mean = np.mean(a, axis=0)\n",
        "#  n = 0\n",
        "#  for i in range(len(a)):\n",
        "#    n = n + a[i]\n",
        "#  mean = n/len(a)\n",
        "#  n = 0\n",
        "#  for i in range(len(a)):\n",
        "#    n = n + ((a[i]-mean) *(a[i]-mean))\n",
        "#  std = np.sqrt(n/len(a))\n",
        "  #print(mean, std)\n",
        "#  n = []\n",
        "#  for i in range(len(a)):\n",
        "#    if a[i] < mean - (0.8 * std) or a[i] > mean + (0.8 * std):\n",
        "#      n.append(1)\n",
        "#    else:\n",
        "#      n.append(0)\n",
        "#  g.append(n)\n",
        "\n",
        "  #fig, axs = plt.subplots(1, 1, figsize=(20,20))\n",
        "  #hist = axs.hist(a, np.arange(-0.01, 0.01, 0.0001)) #numpy to create bins over range\n",
        "  #plt.show()"
      ],
      "metadata": {
        "id": "WzzZ-RFWWF9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import numpy as np\n",
        "#b = np.array(g).sum(axis = 0)\n",
        "\n",
        "#for i in range(len(g)):\n",
        "#  b[i] = b[i] / 50.0\n",
        "\n",
        "#for n in range(len(b)):\n",
        "#  if b[n] > 40:\n",
        "#    print(b[n])"
      ],
      "metadata": {
        "id": "cUAu0xxKzUpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing training modules for LSTM"
      ],
      "metadata": {
        "id": "3sUcXcidiLZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_LR = 0.001\n",
        "# construct model and assign it to device\n",
        "rnn = RNNNetwork(1, 64, 20, 1, 501).to(device)\n",
        "print(rnn)\n",
        "# initialise loss funtion + optimiser\n",
        "loss_MSE = torch.nn.MSELoss()\n",
        "optimizer_LSTM = torch.optim.Adam(rnn.parameters(), lr=LSTM_LR)\n",
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max = 50, # Maximum number of iterations.\n",
        "                             eta_min = 8e-4) # Minimum learning rate.\n",
        "for param in rnn.parameters():\n",
        "    param.requires_grad = True\n",
        "# train model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pRX6fMQiRPM",
        "outputId": "bb20e964-c9e9-4052-e5f9-6f0b893da4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNNetwork(\n",
            "  (lstm): LSTM(64, 20, batch_first=True)\n",
            "  (fc_1): Linear(in_features=20, out_features=128, bias=True)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training rnn"
      ],
      "metadata": {
        "id": "EK3EA61gi24k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbZ1S_i5WyRA",
        "outputId": "1bcb230a-45f1-42f9-cc00-f37dbe136bdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "0.30676851743553185 0.2481914420922597 49.55813953488372 52.12\n",
            "Epoch 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.2460929601739844 0.24521150767803193 54.22714981070849 54.29333333333334\n",
            "Epoch 3\n",
            "0.24132559389246677 0.23907285749912263 56.5040562466198 58.13333333333333\n",
            "Epoch 4\n",
            "0.23393459056376636 0.23190237840016684 59.32341806381828 61.12\n",
            "Epoch 5\n",
            "0.22734385508824323 0.2272629177570343 61.37155219037317 62.44\n",
            "Epoch 6\n",
            "0.22111096185178225 0.2289886099100113 62.44835045970795 68.6\n",
            "Epoch 7\n",
            "0.21807989904434374 0.21688061197598776 64.86425094645755 65.56\n",
            "Epoch 8\n",
            "0.21081117904463478 0.21279805580774944 66.1541373715522 66.68\n",
            "Epoch 9\n",
            "0.20597015307554364 0.20966864705085755 67.37155219037318 65.44\n",
            "Epoch 10\n",
            "0.2008904316993969 0.20918026089668273 68.52298539751217 65.86666666666666\n",
            "Epoch 11\n",
            "0.19724324991117625 0.20332435925801595 69.42347214710654 68.29333333333334\n",
            "Epoch 12\n",
            "0.1948506136466646 0.20034197628498077 70.41373715521904 69.14666666666666\n",
            "Epoch 13\n",
            "0.19121488119667449 0.2032374890645345 71.1308815575987 68.6\n",
            "Epoch 14\n",
            "0.18859607058644232 0.19784597396850587 71.84640346133045 69.32\n",
            "Epoch 15\n",
            "0.18591574971549507 0.1954332427183787 72.25419145484045 70.89333333333333\n",
            "Epoch 16\n",
            "0.18349674679318784 0.19587118526299793 72.79989183342347 69.05333333333333\n",
            "Epoch 17\n",
            "0.18113021259213732 0.19193876246611277 73.50946457544619 70.33333333333333\n",
            "Epoch 18\n",
            "0.17805774780690445 0.18659212191899618 73.93455922120064 71.88\n",
            "Epoch 19\n",
            "0.17334628579035005 0.188555215994517 74.20173066522445 70.68\n",
            "Epoch 20\n",
            "0.16437179070185687 0.18004660467306774 74.75932936722553 71.53333333333333\n",
            "Epoch 21\n",
            "0.15754942474267236 0.1771416332324346 75.45970795024337 72.68\n",
            "Epoch 22\n",
            "0.1533022965448105 0.16814076860745747 76.43861546782044 74.13333333333334\n",
            "Epoch 23\n",
            "0.1480875905782484 0.16644678990046183 78.65386695511087 73.70666666666666\n",
            "Epoch 24\n",
            "0.1440302538043296 0.16352640877167385 79.3731746890211 76.32\n",
            "Epoch 25\n",
            "0.14133331427837204 0.16706878662109376 80.20173066522445 73.66666666666667\n",
            "Epoch 26\n",
            "0.1368691749737803 0.15498296876748402 80.45970795024337 77.56\n",
            "Epoch 27\n",
            "0.13245321683315667 0.16011731465657553 81.39643050297458 76.01333333333334\n",
            "Epoch 28\n",
            "0.13008002392854995 0.1614509423573812 82.0481341265549 75.29333333333334\n",
            "Epoch 29\n",
            "0.12825367677898392 0.1633434542020162 82.44456462952948 75.72\n",
            "Epoch 30\n",
            "0.12496245569026167 0.17120763003826142 82.92861005949162 75.58666666666667\n",
            "Epoch 31\n",
            "0.12192992480817262 0.1613746080795924 83.40346133044889 76.30666666666667\n",
            "Epoch 32\n",
            "0.11979927274908743 0.15332394162813823 83.89832341806382 78.45333333333333\n",
            "Epoch 33\n",
            "0.11740818755119926 0.15962034861246746 84.2520281233099 78.28\n",
            "Epoch 34\n",
            "0.11436502113656265 0.1616572618484497 84.35749053542456 77.38666666666667\n",
            "Epoch 35\n",
            "0.11425379691912459 0.1634114384651184 84.64250946457544 76.70666666666666\n",
            "Epoch 36\n",
            "0.11233848036786168 0.16618067423502605 84.87506760411033 76.14666666666666\n",
            "Epoch 37\n",
            "0.11137362563123311 0.15118664622306824 84.88480259599784 79.38666666666667\n",
            "Epoch 38\n",
            "0.11029266455629312 0.15581110815207164 85.21957815035154 79.25333333333333\n",
            "Epoch 39\n",
            "0.1078900199578929 0.15882282396157582 85.09248242293131 77.16\n",
            "Epoch 40\n",
            "0.10681104376356043 0.1572363636891047 85.68685776095187 76.86666666666666\n",
            "Epoch 41\n",
            "0.10663822278775287 0.15997488915920258 85.74202271498108 77.29333333333334\n",
            "Epoch 42\n",
            "0.1047562773466884 0.1579324511686961 85.8788534342888 78.14666666666666\n",
            "Epoch 43\n",
            "0.10505396854223079 0.14776847998301187 86.02704164413197 79.98666666666666\n",
            "Epoch 44\n",
            "0.10146837393018347 0.15678472598393758 86.25581395348837 77.8\n",
            "Epoch 45\n",
            "0.10057419820564253 0.14898560881614686 86.65765278528934 78.61333333333333\n",
            "Epoch 46\n",
            "0.09810025369866336 0.14582742830117543 86.763115197404 78.57333333333334\n",
            "Epoch 47\n",
            "0.09582462379776381 0.14514944493770598 87.33098972417523 78.86666666666666\n",
            "Epoch 48\n",
            "0.09300020420112888 0.15718188643455505 87.45375878853434 78.88\n",
            "Epoch 49\n",
            "0.09198331593935266 0.14141617894172667 87.63277447268794 79.26666666666667\n",
            "Epoch 50\n",
            "0.08974178097920524 0.15143977502981823 87.83071930773392 81.42666666666666\n",
            "Epoch 51\n",
            "0.08768044791716766 0.13556716352701187 88.29421308815576 80.58666666666667\n",
            "Epoch 52\n",
            "0.08691066300437024 0.13771807213624318 88.62898864250947 80.96\n",
            "Epoch 53\n",
            "0.08361732364184409 0.14148941953976948 89.0811249323959 81.0\n",
            "Epoch 54\n",
            "0.08448721146490072 0.13860998610655467 88.65548945375879 79.30666666666667\n",
            "Epoch 55\n",
            "0.08122961814658844 0.13888589908679327 89.26122228231476 79.29333333333334\n",
            "Epoch 56\n",
            "0.07830711544817238 0.1377698649962743 89.77879935100054 81.13333333333334\n",
            "Epoch 57\n",
            "0.08077178338593957 0.13482593854268393 89.30611141157382 80.54666666666667\n",
            "Epoch 58\n",
            "0.07837976429867319 0.1377263073126475 89.76149269875609 80.14666666666666\n",
            "Epoch 59\n",
            "0.07552237072108306 0.14582287351290385 90.17685235262304 80.32\n",
            "Epoch 60\n",
            "0.07567363903289101 0.14424900392691295 90.09410492157923 80.45333333333333\n",
            "Epoch 61\n",
            "0.07318793001885411 0.13451563994089763 90.31368307193077 80.58666666666667\n",
            "Epoch 62\n",
            "0.07131321163693913 0.13368479549884796 90.52298539751217 81.13333333333334\n",
            "Epoch 63\n",
            "0.06872597535021438 0.1315374412139257 90.82855597620335 81.13333333333334\n",
            "Epoch 64\n",
            "0.06956927942916985 0.1390800505876541 90.81719848566793 82.72\n",
            "Epoch 65\n",
            "0.06806300350818975 0.13645987888177236 91.32125473228773 81.56\n",
            "Epoch 66\n",
            "0.06764177545671402 0.1313269676764806 91.46133044889129 81.85333333333334\n",
            "Epoch 67\n",
            "0.06555187981827572 0.12319334089756012 91.53272038939967 82.84\n",
            "Epoch 68\n",
            "0.06580105655020221 0.1254963743686676 91.61546782044348 83.13333333333334\n",
            "Epoch 69\n",
            "0.06415347202709715 0.13076248745123545 91.80205516495403 81.98666666666666\n",
            "Epoch 70\n",
            "0.06272355356470774 0.12251942962408066 92.21362898864251 83.30666666666667\n",
            "Epoch 71\n",
            "0.06438009409267172 0.12730628510316214 91.87128177393186 82.41333333333333\n",
            "Epoch 72\n",
            "0.06721929817376361 0.11565190315246582 91.4808004326663 82.97333333333333\n",
            "Epoch 73\n",
            "0.06209820007281796 0.11654324303070704 92.03461330448891 84.42666666666666\n",
            "Epoch 74\n",
            "0.06361575910429686 0.11959551572799683 92.06165494862087 82.25333333333333\n",
            "Epoch 75\n",
            "0.06026176672696809 0.11779868880907694 92.33856138453218 83.13333333333334\n",
            "Epoch 76\n",
            "0.06030601763915474 0.11943512280782063 92.32287723093565 82.88\n",
            "Epoch 77\n",
            "0.05885887577557577 0.11937160829703013 92.67279610600325 83.6\n",
            "Epoch 78\n",
            "0.05701661177015807 0.1217727925380071 92.796106003245 83.69333333333333\n",
            "Epoch 79\n",
            "0.05657133687511207 0.12247666358947754 92.81936181719848 82.68\n",
            "Epoch 80\n",
            "0.055939231416449284 0.12101459403832754 92.84045429962141 83.6\n",
            "Epoch 81\n",
            "0.05425116805417142 0.12329410413901011 93.11736073553271 82.41333333333333\n",
            "Epoch 82\n",
            "0.05296854920213838 0.1233036873737971 93.29421308815576 82.84\n",
            "Epoch 83\n",
            "0.05485166643941125 0.12366849978764852 93.31530557057869 83.86666666666666\n",
            "Epoch 84\n",
            "0.052848172385654556 0.12275129566589991 93.3829096809086 83.56\n",
            "Epoch 85\n",
            "0.05328998259349795 0.11821753084659577 93.34396971335858 84.04\n",
            "Epoch 86\n",
            "0.05250851588757893 0.12809715976317723 93.29421308815576 82.45333333333333\n",
            "Epoch 87\n",
            "0.05260991613773993 0.12998188416163126 93.51541373715521 82.72\n",
            "Epoch 88\n",
            "0.05382861627145933 0.11870188395182292 93.56354786371011 84.85333333333334\n",
            "Epoch 89\n",
            "0.0520370285444288 0.125062628587087 93.39426717144403 82.41333333333333\n",
            "Epoch 90\n",
            "0.051576190615519116 0.11911009271939596 93.71335857220119 83.44\n",
            "Epoch 91\n",
            "0.05259358196072542 0.1268838709592819 93.796106003245 82.45333333333333\n",
            "Epoch 92\n",
            "0.050070043679441616 0.12680974423885347 93.80205516495403 83.44\n",
            "Epoch 93\n",
            "0.049690679377554364 0.12426973183949788 93.79448350459708 83.98666666666666\n",
            "Epoch 94\n",
            "0.048327062605377016 0.12865365236997606 94.01730665224446 82.74666666666667\n",
            "Epoch 95\n",
            "0.048067177481155514 0.1244092599550883 93.99242833964306 83.01333333333334\n",
            "Epoch 96\n",
            "0.04830816953874653 0.13233945826689403 93.85181179015684 82.54666666666667\n",
            "Epoch 97\n",
            "0.05243789269843251 0.1251336677869161 93.5040562466198 83.98666666666666\n",
            "Epoch 98\n",
            "0.04828998883465872 0.12335136493047079 93.796106003245 83.09333333333333\n",
            "Epoch 99\n",
            "0.0470858842090151 0.1233661139011383 94.12493239588967 82.76\n",
            "Epoch 100\n",
            "0.045977030441027966 0.12167131066322327 94.15575987020011 83.57333333333334\n",
            "Epoch 101\n",
            "0.04536530438945576 0.13354318976402282 94.20010816657653 82.41333333333333\n",
            "Epoch 102\n",
            "0.047332662472520534 0.12857241064310074 94.21362898864251 83.69333333333333\n",
            "Epoch 103\n",
            "0.04625381020000008 0.11890350311994552 94.49432125473228 84.16\n",
            "Epoch 104\n",
            "0.04438257255889133 0.11301505645116171 94.64629529475393 84.58666666666667\n",
            "Epoch 105\n",
            "0.044922269979494854 0.12195029596487682 94.34613304488913 82.70666666666666\n",
            "Epoch 106\n",
            "0.04559906163510276 0.1334019031127294 94.25365062195782 82.29333333333334\n",
            "Epoch 107\n",
            "0.04594473637088432 0.12751904845237733 94.35586803677664 81.0\n",
            "Epoch 108\n",
            "0.04573724485567095 0.1293966816862424 94.50189291508924 83.18666666666667\n",
            "Epoch 109\n",
            "0.04690692810721498 0.12527851144472757 94.3980530016225 83.86666666666666\n",
            "Epoch 110\n",
            "0.045917397063767476 0.1285594226916631 94.30394808004327 81.48\n",
            "Epoch 111\n",
            "0.0437551342825299 0.12664552410443625 94.61925365062196 82.88\n",
            "Epoch 112\n",
            "0.04277297133139625 0.13314513832330704 94.78474851270957 82.29333333333334\n",
            "Epoch 113\n",
            "0.04262250780720785 0.12627196371555327 94.75770686857761 82.2\n",
            "Epoch 114\n",
            "0.042369555669855334 0.1315426081418991 94.74418604651163 81.56\n",
            "Epoch 115\n",
            "0.04221450120992374 0.12422356645266215 94.75770686857761 81.98666666666666\n",
            "Epoch 116\n",
            "0.04122817693424263 0.13098235428333282 95.11357490535424 81.85333333333334\n",
            "Epoch 117\n",
            "0.04191936296650111 0.1247735075155894 94.90968090859924 82.70666666666666\n",
            "Epoch 118\n",
            "0.04137081990956357 0.12036074231068293 95.09031909140076 84.0\n",
            "Epoch 119\n",
            "0.04530510940806101 0.12165720641613006 94.59816116819903 85.10666666666667\n",
            "Epoch 120\n",
            "0.042640457722610756 0.1297468755642573 94.89994591671173 81.69333333333333\n",
            "Epoch 121\n",
            "0.04091652526669789 0.12972126285235086 95.16170903190914 81.85333333333334\n",
            "Epoch 122\n",
            "0.04220133052364177 0.13458462238311766 95.00757166035694 82.66666666666667\n",
            "Epoch 123\n",
            "0.04029792432586137 0.13021397729714712 95.08653326122229 82.28\n",
            "Epoch 124\n",
            "0.03972524691226032 0.134801218410333 95.22120064899946 81.73333333333333\n",
            "Epoch 125\n",
            "0.042593578877015134 0.13142246584097544 95.11357490535424 83.56\n",
            "Epoch 126\n",
            "0.040279542448038406 0.12655408481756847 95.20010816657653 82.41333333333333\n",
            "Epoch 127\n",
            "0.039267035199177146 0.128338516553243 95.43861546782044 81.69333333333333\n",
            "Epoch 128\n",
            "0.03675910773177834 0.12441139022509257 95.54624121146566 82.70666666666666\n",
            "Epoch 129\n",
            "0.03901304576592293 0.13032918413480124 95.32882639264467 82.70666666666666\n",
            "Epoch 130\n",
            "0.038187629319606536 0.12599615136782327 95.47701460248783 82.32\n",
            "Epoch 131\n",
            "0.03715939687443076 0.1222504018743833 95.75770686857761 81.44\n",
            "Epoch 132\n",
            "0.035431064935295305 0.12646429747343063 95.72687939426717 82.28\n",
            "Epoch 133\n",
            "0.03589995362553614 0.11856115559736888 95.67712276906435 81.86666666666666\n",
            "Epoch 134\n",
            "0.035987103276313354 0.11936796228090922 95.8058409951325 83.73333333333333\n",
            "Epoch 135\n",
            "0.03929245197947055 0.12664908478657405 95.56733369388859 83.01333333333334\n",
            "Epoch 136\n",
            "0.03802512577352587 0.11946370194355647 95.53272038939967 84.0\n",
            "Epoch 137\n",
            "0.0364599303176688 0.12092450499534607 95.6749594375338 83.44\n",
            "Epoch 138\n",
            "0.039102776005275834 0.11735635459423065 95.46349378042186 84.29333333333334\n",
            "Epoch 139\n",
            "0.042738187484938815 0.12299527058998744 95.12709572742023 82.45333333333333\n",
            "Epoch 140\n",
            "0.03746323062136181 0.12462034086386363 95.55597620335317 83.01333333333334\n",
            "Epoch 141\n",
            "0.036437642933340446 0.12769352624813715 95.75770686857761 83.86666666666666\n",
            "Epoch 142\n",
            "0.03662769880856998 0.1162573966383934 95.67117360735533 85.86666666666666\n",
            "Epoch 143\n",
            "0.03554352012444278 0.11638952155907949 95.77501352082207 82.88\n",
            "Epoch 144\n",
            "0.03585854232645699 0.12329142580429713 95.66738777717686 82.88\n",
            "Epoch 145\n",
            "0.03371710893574632 0.1175722994407018 95.82693347755543 83.13333333333334\n",
            "Epoch 146\n",
            "0.032436182011239205 0.1176953187584877 96.06544077879936 84.42666666666666\n",
            "Epoch 147\n",
            "0.032092893996823475 0.12126112083594004 96.11357490535424 84.41333333333333\n",
            "Epoch 148\n",
            "0.032013889309472174 0.11655869563420614 95.97674418604652 83.13333333333334\n",
            "Epoch 149\n",
            "0.03181962187468941 0.12262391149997712 96.06165494862087 83.01333333333334\n",
            "Epoch 150\n",
            "0.03629757640827921 0.12191052675247192 95.92103839913467 82.70666666666666\n",
            "---------------------------\n",
            "Finished training\n",
            "Trained feed forward net saved at rnnnetwork.pth\n"
          ]
        }
      ],
      "source": [
        "for i in range(EPOCHS):\n",
        "    #try:\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        train_acc = 0\n",
        "        val_acc = 0\n",
        "        n = 0\n",
        "        for input, target in train_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            input = torch.permute(input, (1,0,3,2)).to(device)[0]\n",
        "            # calculate loss\n",
        "            prediction = rnn(input)\n",
        "            loss = loss_MSE(prediction, target)\n",
        "            train_loss = train_loss + (loss.item()*len(input))\n",
        "            # backpropagate error and update weights\n",
        "            optimizer_LSTM.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_LSTM.step()\n",
        "            train_acc = train_acc + (len(input)*calc_accuracy(rnn, input, target))\n",
        "            n = n + len(input)\n",
        "        train_loss/=n\n",
        "        train_acc/=n\n",
        "        n = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            input = torch.permute(input, (1,0,3,2))[0]\n",
        "            # calculate loss\n",
        "            prediction = rnn(input)\n",
        "            loss = loss_MSE(prediction, target)\n",
        "            val_loss = val_loss + (loss.item()*len(input))\n",
        "            val_acc = val_acc + (len(input)*calc_accuracy(rnn, input, target))\n",
        "            n = n + len(input)\n",
        "        val_loss/=n\n",
        "        val_acc/=n\n",
        "        writer.add_scalar('LSTM_Loss/train', train_loss, i)\n",
        "        writer.add_scalar('LSTM_Loss/validation', val_loss, i)\n",
        "        writer.add_scalar('LSTM_Accuracy/train', train_acc, i)\n",
        "        writer.add_scalar('LSTM_Accuracy/validation', val_acc, i)\n",
        "        print(train_loss, val_loss, train_acc, val_acc)\n",
        "        scheduler.step()\n",
        "    #except:\n",
        "      #break\n",
        "print(\"---------------------------\")\n",
        "print(\"Finished training\")\n",
        "# save model\n",
        "torch.save(rnn.state_dict(), \"rnnnetwork.pth\")\n",
        "print(\"Trained feed forward net saved at rnnnetwork.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating cnn and rnn"
      ],
      "metadata": {
        "id": "pgb_ef2gmGhu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9crsBvC_JHa",
        "outputId": "97340f1e-9b15-4e33-d1fe-83321e83079e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN results: \n",
            "0.06252353542105064 96.84045429962141 0.13559948841730754 93.16 0.13559948792060217 93.41333333333333\n",
            "RNN results: \n",
            "0.5307166346541735 95.6749594375338 0.5788912749290467 82.45333333333333 0.5788912765185038 82.70666666666666\n"
          ]
        }
      ],
      "source": [
        "def evaluate_cnn(model):\n",
        "        n = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            val_loss = val_loss + (loss.item()*len(input))\n",
        "            val_acc = val_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        val_loss/=n\n",
        "        val_acc/=n\n",
        "        n = 0\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for input, target in train_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            train_loss = train_loss + (loss.item()*len(input))\n",
        "            train_acc = train_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        train_loss/=n\n",
        "        train_acc/=n\n",
        "        n = 0\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            test_loss = test_loss + (loss.item()*len(input))\n",
        "            test_acc = test_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        test_loss/=n\n",
        "        test_acc/=n\n",
        "        print(train_loss, train_acc, val_loss, val_acc, test_loss, test_acc)\n",
        "\n",
        "def evaluate_rnn(model):\n",
        "        n = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            input = torch.permute(input, (1,0,3,2)).to(device)[0]\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            val_loss = val_loss + (loss.item()*len(input))\n",
        "            val_acc = val_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        val_loss/=n\n",
        "        val_acc/=n\n",
        "        n = 0\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for input, target in train_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            input = torch.permute(input, (1,0,3,2)).to(device)[0]\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            train_loss = train_loss + (loss.item()*len(input))\n",
        "            train_acc = train_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        train_loss/=n\n",
        "        train_acc/=n\n",
        "        n = 0\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            input = torch.permute(input, (1,0,3,2)).to(device)[0]\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            test_loss = test_loss + (loss.item()*len(input))\n",
        "            test_acc = test_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        test_loss/=n\n",
        "        test_acc/=n\n",
        "        print(train_loss, train_acc, val_loss, val_acc, test_loss, test_acc)\n",
        "\n",
        "print('CNN results: ')\n",
        "evaluate_cnn(cnn)\n",
        "print('RNN results: ')\n",
        "evaluate_rnn(rnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the datasets for preprocessing the unsupervised labelled data"
      ],
      "metadata": {
        "id": "kH6qQcbQjmRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io.wavfile import write\n",
        "import random\n",
        "from IPython.display import Audio, display\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class DysarthriaDataset(Dataset):\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        self.data = pd.read_csv(root_dir + '/data.csv')\n",
        "        self.mat = []\n",
        "        def preprocess(signal, sr, augument=False, visualize=False):\n",
        "                if augument:\n",
        "                    effects = [\n",
        "                        [\"speed\", str(random.uniform(0.5, 1.5))],\n",
        "                        ['pitch', str(random.uniform(-8,8))],\n",
        "                        [\"rate\", f\"{sr}\"],\n",
        "                    ]\n",
        "                    signal, sr = torchaudio.sox_effects.apply_effects_tensor(signal, sr, effects)\n",
        "                if sr != 16000:\n",
        "                    resampler = torchaudio.transforms.Resample(sr, 16000)\n",
        "                    signal = resampler(signal)\n",
        "                if signal.shape[1] > 100000:\n",
        "                    signal = signal[:, :100000]\n",
        "                length_signal = signal.shape[1]\n",
        "                if length_signal < 100000:\n",
        "                    num_missing_samples = 100000 - length_signal\n",
        "                    last_dim_padding = (0, num_missing_samples)\n",
        "                    signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "                if visualize:\n",
        "                  display(Audio(signal, rate=16000))\n",
        "                mfcc = T.MFCC(sample_rate=16000, n_mfcc=64, melkwargs={\"n_mels\": 64})\n",
        "                signal = mfcc(signal)\n",
        "                if visualize:\n",
        "                  plot_spectrogram(signal[0])\n",
        "                return signal\n",
        " \n",
        "        for index in range(2000):\n",
        "            try:\n",
        "                label = torch.tensor([1.0]) if self.data.iloc[index][0] == 'dysarthria' else torch.tensor([0.0])\n",
        "                signal, sr = torchaudio.load('drive/MyDrive/' + self.data.iloc[index][2])\n",
        "                #if label == torch.tensor([1.0]):\n",
        "                  #samp1 = preprocess(signal, sr, visualize=True)\n",
        "                  #samp2 = preprocess(signal, sr, augument=True, visualize=True)\n",
        "                  #samp3 = preprocess(signal, sr, augument=True, visualize=True)\n",
        "                  #samp4 = preprocess(signal, sr, augument=True, visualize=True)\n",
        "                #else:\n",
        "                samp1 = preprocess(signal, sr)\n",
        "                samp2 = preprocess(signal, sr, augument=True)\n",
        "                samp3 = preprocess(signal, sr, augument=True)\n",
        "                samp4 = preprocess(signal, sr, augument=True)\n",
        "                self.mat.append([samp1, label]), self.mat.append([samp2, label]), self.mat.append([samp3, label]), self.mat.append([samp4, label])\n",
        "            except:\n",
        "                print('A corrupt file was found')\n",
        "        cluster_ids = self.get_severity_classes()\n",
        "        n = 0\n",
        "        for index in range(len(self.mat)):\n",
        "          if torch.eq(self.mat[index][1], torch.tensor([1.0])):\n",
        "            self.mat[index][1] = cluster_ids[n] + 1\n",
        "            n = n + 1\n",
        "        for index in range(len(self.mat)):\n",
        "            t = torch.zeros((6))\n",
        "            t[int(self.mat[index][1])] = 1\n",
        "            self.mat[index][1] = t\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.mat)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.mat[index][0], self.mat[index][1]\n",
        "\n",
        "    def get_severity_classes(self):\n",
        "        x = []\n",
        "        for index in range(len(self.mat)):\n",
        "          if torch.eq(self.mat[index][1], torch.tensor([1.])):\n",
        "            x.append(self.mat[index][0].tolist())\n",
        "        x = torch.tensor(x).detach().cpu().numpy()\n",
        "        x = np.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]*x.shape[3]))\n",
        "        pca = PCA(n_components=25).fit(x)\n",
        "        x = pca.transform(x)\n",
        "        print('Variation Explained: ' + str(np.sum(pca.explained_variance_ratio_)))\n",
        "        kmeans = KMeans(n_clusters=5, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
        "        kmeans.fit_predict(x)\n",
        "\n",
        "        fig = plt.figure()\n",
        "        ax = fig.add_subplot(projection='3d')\n",
        "        print(len(kmeans.labels_))\n",
        "        n = 100\n",
        "\n",
        "        # For each set of style and range settings, plot n random points in the box\n",
        "        # defined by x in [23, 32], y in [0, 100], z in [zlow, zhigh].\n",
        "        for i in range(len(x)):\n",
        "          m = x[i]\n",
        "          if kmeans.labels_[i] == 0:\n",
        "            ax.scatter(m[0], m[1], m[2], marker=\"o\", c='#1f77b4')\n",
        "          elif kmeans.labels_[i] == 1:\n",
        "            ax.scatter(m[0], m[1], m[2], marker=\"o\", c='#ff7f0e')\n",
        "          elif kmeans.labels_[i] == 2:\n",
        "            ax.scatter(m[0], m[1], m[2], marker=\"o\", c='#2ca02c')\n",
        "          elif kmeans.labels_[i] == 3:\n",
        "            ax.scatter(m[0], m[1], m[2], marker=\"o\", c='#bcbd22')\n",
        "          elif kmeans.labels_[i] == 4:\n",
        "            ax.scatter(m[0], m[1], m[2], marker=\"o\", c='#d62728')\n",
        "        ax.set_xlabel('X Label')\n",
        "        ax.set_ylabel('Y Label')\n",
        "        ax.set_zlabel('Z Label')\n",
        "\n",
        "        plt.show()\n",
        "        return kmeans.labels_\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 200\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using {device}\")\n",
        "data = DysarthriaDataset('drive/MyDrive/torgo_data')\n",
        "print(len(data))\n",
        "\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(data, [7396,300, 300])\n",
        "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "NzQhmCSPgsj7",
        "outputId": "5ebb32ed-4ed4-4a5f-c1c7-67980ced9301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "A corrupt file was found\n",
            "Variation Explained: 0.8941503\n",
            "3996\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([])\n",
        "output = torch.tensor([])\n",
        "for i in range(len(data)):\n",
        "  mat, id = data.__getitem__(i)\n",
        "  input = torch.cat((input, torch.unsqueeze(mat, dim=0)), dim=0)\n",
        "  output = torch.cat((output, torch.unsqueeze(id, dim=0)), dim=0)"
      ],
      "metadata": {
        "id": "Q-fMJ5nEd8_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(input, 'unsupInputs.pt')\n",
        "torch.save(output, 'unsupLabels.pt')"
      ],
      "metadata": {
        "id": "n15N3zRTimIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing training modules for CRNN"
      ],
      "metadata": {
        "id": "Bdg74xkal54l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 2e-2\n",
        "# construct model and assign it to device\n",
        "crnn = CRNN(6, 640, 20, 3, 33).cuda()\n",
        "print(crnn)\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(crnn.parameters(), lr=LEARNING_RATE)\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "scheduler = CosineAnnealingLR(optimizer,\n",
        "                              T_max = 50, # Maximum number of iterations.\n",
        "                             eta_min = 8e-4) # Minimum learning rate.\n",
        "for param in crnn.parameters():\n",
        "    param.requires_grad = True\n",
        "# train model"
      ],
      "metadata": {
        "id": "u6zIhwp0hUtH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5587873-c2a3-43dd-d8bb-c287a1cc44e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (lstm): LSTM(640, 20, num_layers=3, batch_first=True)\n",
            "  (fc1): Linear(in_features=660, out_features=6, bias=True)\n",
            "  (relu): ReLU()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training crnn"
      ],
      "metadata": {
        "id": "b10JzIHhl7WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy(mdl: torch.nn.Module, X: torch.Tensor, Y: torch.Tensor) -> float:\n",
        "        predicted = mdl(X)\n",
        "        correct = 0\n",
        "        smax = nn.Softmax()\n",
        "        predicted = smax(predicted)\n",
        "        for i in range(len(X)):\n",
        "          if torch.sum(torch.eq(Y[i], torch.round(predicted[i])).long()) == 6:\n",
        "            correct = correct + 1\n",
        "        return 100 * correct // len(X)\n",
        "\n",
        "EPOCHS = 100\n",
        "for i in range(EPOCHS):\n",
        "    #try:\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_loss = 0\n",
        "        val_loss = 0\n",
        "        train_acc = 0\n",
        "        val_acc = 0\n",
        "        n = 0\n",
        "        for input, target in train_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = crnn(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            train_loss = train_loss + (loss.item()*len(input))\n",
        "            # backpropagate error and update weights\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_acc = train_acc + (len(input)*calc_accuracy(crnn, input, target))\n",
        "            n = n + len(input)\n",
        "        train_loss/=n\n",
        "        train_acc/=n\n",
        "        n = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = crnn(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            val_loss = val_loss + (loss.item()*len(input))\n",
        "            val_acc = val_acc + (len(input)*calc_accuracy(crnn, input, target))\n",
        "            n = n + len(input)\n",
        "        val_loss/=n\n",
        "        val_acc/=n\n",
        "        writer.add_scalar('CRNN_Loss/train', train_loss, i)\n",
        "        writer.add_scalar('CRNN_Loss/validation', val_loss, i)\n",
        "        writer.add_scalar('CRNN_Accuracy/train', train_acc, i)\n",
        "        writer.add_scalar('CRNN_Accuracy/validation', val_acc, i)\n",
        "        print(train_loss, val_loss, train_acc, val_acc)\n",
        "        scheduler.step()\n",
        "    #except:\n",
        "    #  break\n",
        "print(\"---------------------------\")\n",
        "print(\"Finished training\")\n",
        "# save model\n",
        "torch.save(crnn.state_dict(), \"crnnnetwork.pth\")\n",
        "print(\"Trained feed forward net saved at crnnnetwork.pth\")"
      ],
      "metadata": {
        "id": "jo8CaKachhxL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "210737f7-5156-4482-ab04-5257aaeaa814"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-922dc3617190>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  predicted = smax(predicted)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5366791189522793 1.4202788321177164 10.74959437533802 41.70666666666666\n",
            "Epoch 2\n",
            "1.4712026225611479 1.4214660008748372 31.99837750135208 6.466666666666667\n",
            "Epoch 3\n",
            "1.4479318468425002 1.3701232941945394 33.25419145484045 36.72\n",
            "Epoch 4\n",
            "1.4139924799835057 1.2852275387446086 36.41535965386696 44.666666666666664\n",
            "Epoch 5\n",
            "1.3655260804538922 1.2592292070388793 37.914007571660356 48.12\n",
            "Epoch 6\n",
            "1.3395591825327402 1.1801172240575155 37.19307733910222 47.093333333333334\n",
            "Epoch 7\n",
            "1.2752241677629297 1.1332238419850666 38.9848566792861 49.6\n",
            "Epoch 8\n",
            "1.238553064200864 1.1050039529800415 39.12655489453759 43.333333333333336\n",
            "Epoch 9\n",
            "1.169713707277103 1.1574543491999307 40.80475932936722 51.653333333333336\n",
            "Epoch 10\n",
            "1.1620404684201906 0.9588477126757304 37.93564088696593 49.48\n",
            "Epoch 11\n",
            "1.0309203893768266 0.9053124237060547 40.80800432666306 48.10666666666667\n",
            "Epoch 12\n",
            "0.9972322391393443 0.9573141105969747 41.12276906435911 36.973333333333336\n",
            "Epoch 13\n",
            "0.9934408656321969 0.8294128092130025 43.22714981070849 53.013333333333335\n",
            "Epoch 14\n",
            "0.8882117866180341 0.7632535584767659 46.9572742022715 51.53333333333333\n",
            "Epoch 15\n",
            "0.8254854598839648 0.8059844501813253 48.7782585181179 46.586666666666666\n",
            "Epoch 16\n",
            "0.7869054010715144 0.621764223575592 50.83504597079502 57.08\n",
            "Epoch 17\n",
            "0.7058475730933648 0.6610446484883626 57.98269334775554 58.57333333333333\n",
            "Epoch 18\n",
            "0.6619437531305687 0.6427905400594075 61.90751757706869 62.70666666666666\n",
            "Epoch 19\n",
            "0.6353534175744889 0.6110880168279013 66.00540832882639 65.69333333333333\n",
            "Epoch 20\n",
            "0.5765153208960063 0.4294700594743093 71.28123309897242 76.26666666666667\n",
            "Epoch 21\n",
            "0.5563048722293972 0.6470101102193196 73.53434288804759 69.57333333333334\n",
            "Epoch 22\n",
            "0.5226821621216072 0.40872745712598163 75.77122769064358 77.29333333333334\n",
            "Epoch 23\n",
            "0.5122942361978662 0.4164728418986003 77.92103839913467 80.88\n",
            "Epoch 24\n",
            "0.44480954235537495 0.48374879558881123 81.59653866955111 76.70666666666666\n",
            "Epoch 25\n",
            "0.41176517801777357 0.3725857587655385 84.05949161709032 84.0\n",
            "Epoch 26\n",
            "0.38411076490204676 0.603247747818629 86.04164413196322 72.01333333333334\n",
            "Epoch 27\n",
            "0.3656053979568316 0.2980044368902842 86.85018929150893 87.02666666666667\n",
            "Epoch 28\n",
            "0.33464018915910343 0.3046931819121043 88.16711736073553 85.86666666666666\n",
            "Epoch 29\n",
            "0.30031978116124436 0.3291735843817393 89.72904272579773 84.0\n",
            "Epoch 30\n",
            "0.29377664057482505 0.29033795634905496 90.51541373715521 87.70666666666666\n",
            "Epoch 31\n",
            "0.26388844957282054 0.2591584126154582 92.1541373715522 90.13333333333334\n",
            "Epoch 32\n",
            "0.25833995262308984 0.2555467919508616 92.44997295835587 89.10666666666667\n",
            "Epoch 33\n",
            "0.23344090257772823 0.23972443441549937 93.06165494862087 90.56\n",
            "Epoch 34\n",
            "0.22239273673718268 0.26594047784805297 93.63277447268794 89.28\n",
            "Epoch 35\n",
            "0.22214430323189951 0.255752698580424 93.75987020010817 88.73333333333333\n",
            "Epoch 36\n",
            "0.1994713158898253 0.23209277788798013 94.44240129799891 90.17333333333333\n",
            "Epoch 37\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-922dc3617190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-922dc3617190>\u001b[0m in \u001b[0;36mcalc_accuracy\u001b[0;34m(mdl, X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-f8d2c0eab7b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mh_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mc_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#internal state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m            \u001b[0;31m# Propagate input through LSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluating crnn"
      ],
      "metadata": {
        "id": "1pJNuzyimJSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model):\n",
        "        n = 0\n",
        "        val_loss = 0\n",
        "        val_acc = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            val_loss = val_loss + (loss.item()*len(input))\n",
        "            val_acc = val_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        val_loss/=n\n",
        "        val_acc/=n\n",
        "        n = 0\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        for input, target in train_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            train_loss = train_loss + (loss.item()*len(input))\n",
        "            train_acc = train_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        train_loss/=n\n",
        "        train_acc/=n\n",
        "        n = 0\n",
        "        test_loss = 0\n",
        "        test_acc = 0\n",
        "        for input, target in val_dataloader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            # calculate loss\n",
        "            prediction = model(input)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            test_loss = test_loss + (loss.item()*len(input))\n",
        "            test_acc = test_acc + (len(input)*calc_accuracy(model, input, target))\n",
        "            n = n + len(input)\n",
        "        test_loss/=n\n",
        "        test_acc/=n\n",
        "        print(train_loss, train_acc, val_loss, val_acc, test_loss, test_acc)\n",
        "print('CRNN results: ')\n",
        "evaluate_model(crnn)"
      ],
      "metadata": {
        "id": "UAyYvn6AmL2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4408ce0-f07f-41f6-b411-ddbfaf9edcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CRNN results: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-106-922dc3617190>:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  predicted = smax(predicted)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16347073532717496 93.20010816657653 0.25558602770169575 88.42666666666666 0.25558609565099083 88.73333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "writer.close()"
      ],
      "metadata": {
        "id": "ie29gn3Ft0bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch torchvision\n",
        "#!pip install tensorboard\n",
        "!tensorboard --logdir=my_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTXou1LGuFLA",
        "outputId": "544d5243-b165-41b7-dafe-f1de30eb005a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-05 10:47:51.944234: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-05 10:47:51.944316: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2023-03-05 10:47:51.944330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
            "TensorBoard 2.11.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/main.py\", line 46, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/absl/app.py\", line 308, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/absl/app.py\", line 254, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/program.py\", line 276, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorboard/program.py\", line 295, in _run_serve_subcommand\n",
            "    server.serve_forever()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/werkzeug/serving.py\", line 770, in serve_forever\n",
            "    self.server_close()\n",
            "  File \"/usr/lib/python3.8/socketserver.py\", line 699, in server_close\n",
            "    def server_close(self):\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}